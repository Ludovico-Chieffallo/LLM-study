###############################################################################
# OBIETTIVO GENERALE DEL NOTEBOOK (RICERCA SEMANTICA / SEMANTIC SEARCH)
###############################################################################
# Questo notebook fa (in ordine):
# 1) Installa librerie (Colab).
# 2) Importa moduli.
# 3) Carica da disco un "indice" salvato (lista di (embedding, metadati_doc)).
# 4) Carica (o riusa) lo STESSO modello usato per creare quell'indice.
# 5) Prende una query testuale dell’utente e genera l'embedding della query.
# 6) Calcola la similarità coseno tra query e ogni embedding documento.
# 7) Ordina i risultati e stampa i top-k documenti più simili.
#
# Parole chiave:
# - Embedding: vettore numerico che rappresenta il significato di una frase/testo.
# - Cosine similarity: misura quanto due vettori puntano nella stessa direzione.
###############################################################################


###############################################################################
# 0) METADATI / INTESTAZIONE COLAB
###############################################################################
# -*- coding: utf-8 -*-
"""ricerca_semantica_informazioni.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDBy0NwpY6RzGSviPmxPDsHYnV0xCYGz
"""

# --- SPIEGAZIONE MINUZIOSA ---
# # -*- coding: utf-8 -*-
# - Dichiarazione dell'encoding del file sorgente.
# - Serve a dire a Python come interpretare caratteri non-ASCII (accenti, emoji, ecc.).
# - In Python 3 spesso è superfluo, ma nei notebook/ambienti vari è una sicurezza.

# La docstring multi-linea """ ... """
# - È solo testo informativo (commento "grande") su origine del notebook.
# - Non influenza l'esecuzione del codice.


###############################################################################
# 1) INSTALLAZIONE DIPENDENZE (SOLO IN COLAB / NOTEBOOK)
###############################################################################
#!pip install sentence-transformers -q
#!pip install scikit-learn -q # Per cosine_similarity

print("Installazione delle librerie 'sentence-transformers' e 'scikit-learn' completata.")

# --- SPIEGAZIONE MINUZIOSA ---
# !pip install ...
# - Il "!" in Colab/Jupyter indica: "esegui questo comando nella shell", non in Python puro.
# - pip install scarica e installa pacchetti Python nell'ambiente corrente.

# sentence-transformers
# - Libreria che fornisce modelli pre-addestrati per embeddings di frasi.

# scikit-learn
# - Libreria ML classica. Qui la usi per cosine_similarity.

# -q
# - "quiet": riduce output a schermo, utile per non intasare il notebook.

# print(...)
# - Messaggio di conferma: utile perché le installazioni a volte sono lente o falliscono,
#   e vuoi un "checkpoint visivo".


###############################################################################
# 2) IMPORT MODULI
###############################################################################
import pickle
import os
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import torch # Utilizzato per verificare la disponibilità della GPU

print("Moduli necessari importati con successo.")

# --- SPIEGAZIONE MINUZIOSA ---
# import pickle
# - Per caricare l'indice salvato come file .pkl (serializzazione).

# import os
# - Per controllare se esiste il file dell'indice (os.path.exists).

# import numpy as np
# - Utile perché embeddings/reshape spesso sono NumPy.
# - Qui np non viene usato direttamente, ma è coerente con l'ambiente.

# from sentence_transformers import SentenceTransformer
# - Classe per caricare il modello.

# from sklearn.metrics.pairwise import cosine_similarity
# - Funzione pronta per calcolare la similarità coseno tra due set di vettori.

# import torch
# - Serve per verificare CUDA (GPU) e stampare nome GPU in Colab.


###############################################################################
# 3) CONFIG: PERCORSO FILE INDICE E VARIABILI DI STATO
###############################################################################
index_filepath = "my_simple_corpus_index.pkl"
loaded_index = None

print(f"Tentativo di caricare l'indice vettoriale da '{index_filepath}'...")

# --- SPIEGAZIONE MINUZIOSA ---
# index_filepath
# - Nome file dove è stato salvato l'indice in precedenza (lezione 43).

# loaded_index = None
# - Placeholder. None = "non ancora caricato" / "non disponibile".

# print(f"...{index_filepath}...")
# - f-string: permette di inserire variabili dentro la stringa con { }.
# - Serve per debug: se cambi nome file, lo vedi subito nell'output.


###############################################################################
# 4) CARICAMENTO DELL'INDICE DA DISCO
###############################################################################
if os.path.exists(index_filepath):
    try:
        with open(index_filepath, "rb") as f_in:
            loaded_index = pickle.load(f_in)

        if loaded_index:
            print(f"Indice caricato con successo da '{index_filepath}'.")
            print(f"Contiene {len(loaded_index)} elementi (documenti).")
            # Esaminiamo brevemente il primo elemento per conferma
            if len(loaded_index) > 0:
                emb_example, ref_example = loaded_index[0]
                print(f"  Esempio primo embedding shape: {emb_example.shape}")
                print(f"  Riferimento primo documento (ID: {ref_example['id']}): '{ref_example['preview'][:50]}...'")
        else:
            print(f"ATTENZIONE: Indice caricato da '{index_filepath}' ma risulta vuoto o corrotto.")
            print("Potrebbe essere necessario rieseguire la Lezione 43 per ricrearlo.")
    except Exception as e:
        print(f"ERRORE CRITICO durante il caricamento dell'indice da '{index_filepath}': {e}")
        print("Assicurati che il file non sia corrotto o di un formato non compatibile.")
else:
    print(f"ERRORE CRITICO: File indice '{index_filepath}' NON TROVATO.")
    print("Assicurati di aver eseguito la Lezione 43 e salvato l'indice in questa sessione di Colab,")
    print("oppure carica il file manualmente nella sidebar di Colab (cartella 'Files').")

# --- SPIEGAZIONE MINUZIOSA (passo per passo) ---
# if os.path.exists(index_filepath):
# - Evita un FileNotFoundError e ti permette messaggi più chiari.

# try:
# - Protegge l'operazione di I/O e deserializzazione:
#   - file corrotto
#   - versione incompatibile
#   - oggetto pickle non leggibile
#   - permessi mancanti
#   - ecc.

# with open(index_filepath, "rb") as f_in:
# - Apre il file in modalità "read binary".
# - Pickle legge bytes, quindi serve 'rb'.

# loaded_index = pickle.load(f_in)
# - Ricostruisce in RAM l'oggetto salvato.
# - Atteso: lista di tuple (embedding, doc_reference).

# if loaded_index:
# - In Python, una lista vuota è "Falsey", una lista con elementi è "Truthy".
# - Quindi qui entri se l'indice non è vuoto.

# if len(loaded_index) > 0:
# - Ridondante rispetto a "if loaded_index", ma più esplicito.

# emb_example, ref_example = loaded_index[0]
# - unpacking di tuple:
#   loaded_index[0] dovrebbe essere (embedding, riferimento_doc)
# - emb_example: vettore embedding
# - ref_example: dict con metadati (id, preview, text, ...)

# emb_example.shape
# - ti dice dimensione embedding (es. (768,)).
# - ATTENZIONE: può essere (768,) oppure (1,768) a seconda di come è stato salvato.

# ref_example['preview'][:50]
# - stampa un pezzetto della preview.

# else (indice vuoto/corrotto)
# - Qui il codice assume: se la lista è vuota, potrebbe essere corrotto.
# - In realtà potrebbe essere semplicemente vuoto (corpus vuoto), ma didatticamente ok.

# except Exception as e:
# - Qui stampi "ERRORE CRITICO" e dettagli.

# else (file non esiste)
# - In Colab è comune: la sessione riparte e i file spariscono se non li hai su Drive.
# - Per questo suggerisce di ricaricare manualmente.


###############################################################################
# 5) SCELTA DEL MODELLO: DEVE ESSERE LO STESSO USATO PER L'INDICE
###############################################################################
model_name_used_for_index = 'paraphrase-multilingual-mpnet-base-v2'
# model_name_used_for_index = 'all-MiniLM-L6-v2' # Esempio se hai usato questo prima

model = None

print(f"Tentativo di gestire il modello '{model_name_used_for_index}'...")

# --- SPIEGAZIONE MINUZIOSA ---
# Perché "DEVE ESSERE LO STESSO MODELLO"?
# - Perché embeddings creati con modelli diversi NON sono comparabili:
#   - dimensioni diverse (es. 768 vs 384)
#   - geometria dello spazio diversa (anche se dimensione uguale)
# - Se usi modello diverso, cosine similarity non ha significato e/o va in errore.

# model = None
# - Placeholder: "non caricato ancora".

# print(...)
# - log informativo.


###############################################################################
# 6) CARICAMENTO / RIUSO DEL MODELLO (GESTIONE COMPLESSA)
###############################################################################
try:
    reused_existing_model = False
    if 'corpus_model' in locals() and isinstance(corpus_model, SentenceTransformer):
        print(f"INFO: Trovata variabile 'corpus_model' di tipo SentenceTransformer.")
        print(f"      Si tenterà di riutilizzarla, assumendo sia il modello '{model_name_used_for_index}'.")
        print(f"      Se hai cambiato 'model_name_used_for_index' da quando 'corpus_model' è stato creato,")
        print(f"      o se 'corpus_model' non è il modello corretto, considera di non eseguire questa parte o di eliminare 'corpus_model'.")
        model = corpus_model
        reused_existing_model = True
        print(f"Riutilizzo del modello da 'corpus_model'. Nome atteso: '{model_name_used_for_index}'.")

    if not reused_existing_model:
        print(f"Caricamento del modello '{model_name_used_for_index}' da zero (potrebbe richiedere qualche istante)...")
        model = SentenceTransformer(model_name_used_for_index)
        print(f"Modello '{model_name_used_for_index}' inizializzato.")

    if model and isinstance(model, SentenceTransformer):
        print(f"Modello '{model_name_used_for_index}' pronto per l'uso.")
        if torch.cuda.is_available():
            print(f"Il modello dovrebbe utilizzare la GPU: {torch.cuda.get_device_name(0)}")
        else:
            print("Il modello sta utilizzando la CPU.")
    else:
        print(f"ERRORE: Il modello '{model_name_used_for_index}' NON è stato caricato o assegnato correttamente.")
        if reused_existing_model and not isinstance(model, SentenceTransformer):
            print("Il riutilizzo di 'corpus_model' non ha prodotto un oggetto SentenceTransformer valido.")
        elif not model:
             print("La variabile 'model' è ancora 'None'. Il caricamento potrebbe essere fallito silenziosamente o l'eccezione non l'ha gestito.")
        model = None

except Exception as e:
    print(f"ERRORE CRITICO durante il caricamento o la gestione del modello '{model_name_used_for_index}':")
    print(f"Tipo di Errore: {type(e).__name__}")
    print(f"Messaggio: {str(e)}")
    print("-" * 20)
    print("Possibili cause e soluzioni:")
    print(f"1. Nome del modello ('{model_name_used_for_index}') errato o non disponibile (controlla typo e connessione internet).")
    print("2. Problemi di dipendenze: prova ad aggiornare le librerie con il comando in una nuova cella:")
    print("!pip install -U sentence-transformers transformers torch")
    print("3. Memoria insufficiente (RAM o GPU VRAM). Controlla l'utilizzo risorse di Colab.")
    print("4. Se l'errore originale era `object has no attribute 'config'`, questa versione dovrebbe averlo risolto.")
    print("Se l'errore persiste con un messaggio diverso, il problema è nel caricamento stesso del modello.")
    model = None

# --- SPIEGAZIONE MINUZIOSA (logica e motivi) ---
# try:
# - Vuoi catturare errori di caricamento modello (molto comuni in Colab).

# reused_existing_model = False
# - Flag booleano: serve per ricordare se hai riutilizzato un modello già esistente.

# if 'corpus_model' in locals() and isinstance(corpus_model, SentenceTransformer):
# - locals() è un dizionario con le variabili nel contesto locale della cella/notebook.
# - Controlli se esiste una variabile chiamata corpus_model
# - e se è del tipo giusto (SentenceTransformer).
#
# PERCHÉ FARE QUESTA COSA?
# - In notebook, spesso esegui celle a caso.
# - Se hai già caricato il modello prima, ricaricarlo è tempo sprecato.

# MA C'È UN PROBLEMA DI DESIGN QUI:
# - Stai "assumendo" che corpus_model sia proprio il modello corretto.
# - Non verifichi davvero che il nome del modello corrisponda a model_name_used_for_index.
#   (SentenceTransformer non espone sempre un campo semplice per il "nome".)
# - Quindi questa parte è comoda ma potenzialmente pericolosa.

# model = corpus_model
# - Assegni alla variabile model l'oggetto già esistente.

# reused_existing_model = True
# - Segni che hai riutilizzato.

# if not reused_existing_model:
# - Se non hai trovato un modello riutilizzabile, lo carichi da zero.

# model = SentenceTransformer(model_name_used_for_index)
# - Caricamento vero.

# if model and isinstance(model, SentenceTransformer):
# - Doppio controllo:
#   - model non è None
#   - è del tipo giusto
# - È un "paracadute" in più.

# if torch.cuda.is_available():
# - Nota: qui NON sposti esplicitamente il modello su GPU.
# - SentenceTransformer spesso decide da solo device, oppure puoi passare device='cuda'.
# - Qui ti limiti a stampare il nome GPU, come informazione.
#
# ATTENZIONE: la frase "Il modello dovrebbe utilizzare la GPU" è "ottimistica".
# In realtà potrebbe essere ancora su CPU a seconda di config.
# Ma per didattica ci sta.

# else blocco (modello non valido)
# - È un controllo di coerenza: se arrivi qui senza eccezioni, significa che
#   qualcosa è andato storto nel flusso logico.

# except Exception as e:
# - Stampa dettagli e possibili cause.
# - Nota: è molto verbose (didattico) e va bene per imparare.


###############################################################################
# 7) CHECK FINALE: POSSIAMO PROCEDERE?
###############################################################################
print("\n--- Verifica Finale del Setup ---")

if loaded_index and model is not None:
    print("Complimenti! Indice vettoriale e modello di embedding sono stati caricati correttamente.")
    print("Siamo pronti per processare la query e avviare la ricerca semantica.")
    print("--------------------------------------------------------------------")
else:
    print("ERRORE: Impossibile procedere. Indice o modello non validi/caricati.")
    print("--------------------------------------------------------------------")
    print("Per favore, verifica i messaggi di errore nelle celle precedenti e assicurati che:")
    print(f"1. Il file '{index_filepath}' esista e sia stato caricato correttamente.")
    print(f"2. Il modello '{model_name_used_for_index}' sia stato caricato correttamente.")
    print("Potrebbe essere necessario rieseguire le celle precedenti o parti della Lezione 43.")

# --- SPIEGAZIONE MINUZIOSA ---
# print("\n--- ... ---")
# - separatore visivo per rendere leggibile output.

# if loaded_index and model is not None:
# - Condizione "tutto ok":
#   - loaded_index deve essere truthy (lista non vuota)
#   - model deve esistere

# else:
# - Se uno dei due manca, non puoi fare semantic search perché:
#   - senza indice non hai documenti con cui confrontare
#   - senza modello non puoi embeddare la query


###############################################################################
# 8) CREAZIONE EMBEDDING DELLA QUERY
###############################################################################
query_embedding = None

if model and loaded_index:
    query = "Quali animali sono considerati i migliori amici dell'uomo?"

    print(f"Query dell'utente: '{query}'")

    try:
        query_embedding = model.encode([query])
        print(f"Embedding della query generato. Shape: {query_embedding.shape}")
        print(f"Primi 3 valori dell'embedding della query: {query_embedding[0][:3]}")
    except Exception as e:
        print(f"ERRORE: Impossibile generare l'embedding per la query: {e}")
        query_embedding = None
else:
    print("Impossibile processare la query: Il modello o l'indice non sono disponibili.")
    print("Assicurati di aver eseguito con successo tutte le celle del 'Passo 1'.")

# --- SPIEGAZIONE MINUZIOSA ---
# query_embedding = None
# - Placeholder.

# if model and loaded_index:
# - Prosegui solo se entrambi sono disponibili.

# query = "..."
# - Query di esempio. In un'app reale arriverebbe dall'utente (input, API, UI).

# model.encode([query])
# - IMPORTANTISSIMO: passi una LISTA con una sola stringa.
# - Perché?
#   - encode() può restituire:
#       * un vettore 1D (D,) se passi una stringa singola
#       * una matrice 2D (N, D) se passi una lista di stringhe
# - sklearn cosine_similarity si aspetta matrici 2D (n_samples, n_features).
# - Quindi usare [query] è una scelta di compatibilità e semplicità.
#
# Risultato tipico:
# - query_embedding.shape == (1, D)

# query_embedding[0][:3]
# - query_embedding[0] = vettore 1D della query (lunghezza D).
# - [:3] = primi 3 valori di preview.

# except
# - gestisce errori (es. modello non disponibile, input strano, ecc).


###############################################################################
# 9) CALCOLO SIMILARITÀ (QUERY vs OGNI DOCUMENTO)
###############################################################################
all_similarities = []

if loaded_index and query_embedding is not None:
    print(f"Avvio del calcolo delle similarità tra la query e i {len(loaded_index)} documenti nell'indice...")

    for i, (doc_embedding, doc_reference) in enumerate(loaded_index):
        if (i + 1) % 5 == 0 or i == 0 or (i + 1) == len(loaded_index):
            print(f"  Elaborazione documento {i+1}/{len(loaded_index)}...")

        current_doc_embedding_reshaped = doc_embedding.reshape(1, -1)

        similarity_score_array = cosine_similarity(query_embedding, current_doc_embedding_reshaped)

        score = similarity_score_array[0][0]

        all_similarities.append( (score, doc_reference) )

    print(f"Calcolo delle similarità completato per tutti i {len(all_similarities)} documenti.")

else:
    print("Impossibile calcolare le similarità: Indice o embedding della query non disponibili.")
    print("Verifica l'output delle celle precedenti (Passo 1 e Passo 2).")

# --- SPIEGAZIONE MINUZIOSA (qui c’è il cuore della ricerca) ---
# all_similarities = []
# - Lista che conterrà risultati: (punteggio, riferimento_doc).

# if loaded_index and query_embedding is not None:
# - loaded_index deve esserci
# - query_embedding deve essere stato creato

# for i, (doc_embedding, doc_reference) in enumerate(loaded_index):
# - Ogni voce dell'indice è attesa come:
#   (embedding, reference)
# - Unpacking:
#   doc_embedding = vettore documento (tipicamente shape (D,))
#   doc_reference = dict metadati (id, text, preview...)

# if (i + 1) % 5 == 0 or i == 0 or (i + 1) == len(loaded_index):
# - Solo logging: non vuoi stampare per ogni documento se sono tanti.
# - Stampa:
#   - al primo (i==0)
#   - ogni 5
#   - all'ultimo
# - Motivo: feedback di avanzamento.

# current_doc_embedding_reshaped = doc_embedding.reshape(1, -1)
# - Questa riga è fondamentale per compatibilità con scikit-learn.
# - reshape(1, -1) significa:
#   - voglio 1 riga
#   - e "-1" significa: "calcola automaticamente quante colonne servono"
# - Se doc_embedding è (D,), diventa (1, D).

# similarity_score_array = cosine_similarity(query_embedding, current_doc_embedding_reshaped)
# - cosine_similarity prende due matrici 2D:
#   A shape (nA, D)
#   B shape (nB, D)
# - Qui:
#   query_embedding shape (1, D)
#   doc_embedding reshaped shape (1, D)
# - Output shape (1, 1): una matrice 1x1 con un solo valore.

# score = similarity_score_array[0][0]
# - Estrai lo scalare (il numero) dalla matrice 1x1.

# all_similarities.append((score, doc_reference))
# - Salvi risultato.
# - Nota: non salvi embedding, perché dopo non ti serve per stampare.
# - Tieni solo punteggio + metadati doc.

# Complessità:
# - Questo è un approccio "brute force": confronti la query con TUTTI i documenti.
# - Va bene per poche centinaia/migliaia.
# - Per milioni serve un index vettoriale (FAISS, Annoy, ScaNN, ecc).


###############################################################################
# 10) ORDINAMENTO RISULTATI E STAMPA TOP-K
###############################################################################
if all_similarities:
    all_similarities.sort(key=lambda item: item[0], reverse=True)

    print(f"\n--- Risultati della Ricerca Semantica per la query: '{query}' ---")

    num_results_to_show = 3
    print(f"Top {num_results_to_show} documenti più simili (su {len(all_similarities)} totali):")

    for i, (score, ref) in enumerate(all_similarities[:num_results_to_show]):
        print(f"\n{i+1}. Punteggio Similarità: {score:.4f}")
        print(f"   ID Documento: {ref['id']}")
        print(f"   Testo Documento: {ref['text']}")
        print("-" * 40)

    if len(all_similarities) > num_results_to_show:
        print(f"\n--- Esempio del documento meno simile ---")
        score_last, ref_last = all_similarities[-1]
        print(f"Ultimo. Punteggio Similarità: {score_last:.4f}")
        print(f"   ID Documento: {ref_last['id']}")
        print(f"   Testo Documento: {ref_last['text']}")
        print("-" * 40)

elif loaded_index and query_embedding is not None:
    print("\nNessuna similarità calcolata. La lista 'all_similarities' è vuota.")
    print("Questo potrebbe accadere se l'indice 'loaded_index' è vuoto o se c'è stato un errore nel calcolo.")
else:
    print("\nImpossibile visualizzare i risultati: Prerequisiti non soddisfatti (indice o query non disponibili).")
    print("Assicurati di aver eseguito con successo tutte le celle precedenti.")

# --- SPIEGAZIONE MINUZIOSA ---
# if all_similarities:
# - Se hai calcolato almeno un punteggio.

# all_similarities.sort(key=lambda item: item[0], reverse=True)
# - sort ordina la lista IN-PLACE (modifica la lista stessa).
# - key=... dice "in base a cosa ordino?"
# - lambda item: item[0]
#   - item è una tupla (score, ref)
#   - item[0] è lo score
# - reverse=True: ordine decrescente (più grande = più simile in alto).

# num_results_to_show = 3
# - k dei risultati top-k.

# all_similarities[:num_results_to_show]
# - slicing: prende i primi k elementi (già ordinati).

# print(f"{score:.4f}")
# - Formattazione float a 4 decimali: leggibilità.

# if len(all_similarities) > num_results_to_show:
# - Se ci sono più documenti, mostra anche il meno simile (ultimo)
# - Per confronto didattico.

# elif loaded_index and query_embedding is not None:
# - Caso raro: prerequisiti ok ma lista vuota (indice vuoto).

# else finale:
# - Mancano prerequisiti => non puoi mostrare risultati.


###############################################################################
# NOTE IMPORTANTI / "PERCHÉ SI FA COSÌ?"
###############################################################################
# 1) Perché usare cosine similarity?
# - Gli embeddings sono vettori in uno spazio semantico.
# - In questi spazi, spesso la "direzione" conta più della "lunghezza".
# - La cosine similarity misura l'angolo tra vettori (quanto sono allineati).

# 2) Perché reshape(1, -1)?
# - sklearn lavora in termini di "matrici di campioni".
# - Anche se hai un singolo vettore, devi presentarlo come matrice 2D.

# 3) Perché salvare con pickle?
# - Facilissimo: salva oggetti Python così come sono.
# - Contro: non è un formato portabile/standard e NON è sicuro da fonti esterne.

# 4) Perché questo NON è un vero "vector database"?
# - Qui fai confronto con tutti i documenti: O(N) per query.
# - Un DB vettoriale usa strutture (ANN) per fare ricerca più veloce su grandi N.
###############################################################################